{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ddd5c-272d-4da8-a977-c409a9eaebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Remove duplicates based on a key column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf468ff-f038-4b4e-a3c4-503fc71c6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "winspec = Window.partitionBy(\"customer_id\") \n",
    "df2 = df.withColumn(\"rn\",row_number().over(winspec))\n",
    "corrected_df = df2.where(F.col(\"rn\") == 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef286983-8cb2-45e9-8323-c400e7007a37",
   "metadata": {},
   "source": [
    "## Explode a nested column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea514fb0-eabd-4999-ae8e-825121471ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user': 1, 'events': [{'type': 'click', 'ts': 100}, {'type': 'purchase', 'ts': 200}]}\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"user\": 1,\n",
    "  \"events\": [\n",
    "    {\"type\": \"click\", \"ts\": 100},\n",
    "    {\"type\": \"purchase\", \"ts\": 200}\n",
    "  ]\n",
    "}\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e983d4d0-1da7-4011-9fd2-bfe7d4c841ed",
   "metadata": {},
   "source": [
    "### When data is read from a dictionary to create a dataframe, it is advisable to create a pandas dataframe from the dictionary and then create a spark dataframe out of pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397f508-5981-4ca2-9d4b-4db624cc5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user\", IntegerType(), True),\n",
    "    StructField(\n",
    "        \"events\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"type\", StringType(), True),\n",
    "                StructField(\"ts\", IntegerType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        True\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d554171-923e-4b23-9dc6-45da02b7f6cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/20 21:09:07 WARN Utils: Your hostname, user-HP-Pavilion-x360-Convertible-14-dh0xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface wlo1)\n",
      "25/11/20 21:09:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/20 21:09:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName(\"explode\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500931c-b95b-47ee-bd0a-e6e9b409602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c81ed0a-bbe2-4fdf-a5da-ef24539cc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357d9323-b139-4d43-849d-d1aff7512e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{'type': 'click', 'ts': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'type': 'purchase', 'ts': 200}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                           events\n",
       "0     1     {'type': 'click', 'ts': 100}\n",
       "1     1  {'type': 'purchase', 'ts': 200}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = pd.DataFrame(data)\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9446b158-a5e6-400f-960d-586dbeb39453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------------------+\n",
      "|user|events                       |\n",
      "+----+-----------------------------+\n",
      "|1   |{type -> click, ts -> 100}   |\n",
      "|1   |{type -> purchase, ts -> 200}|\n",
      "+----+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pdf) \n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de23ffd9-4528-432e-be94-3128d15b3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------+\n",
      "|user|type|      ts|\n",
      "+----+----+--------+\n",
      "|   1|type|   click|\n",
      "|   1|  ts|     100|\n",
      "|   1|type|purchase|\n",
      "|   1|  ts|     200|\n",
      "+----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(\"user\", explode(\"events\").alias(\"type\",\"ts\")) \n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd205575-b9c6-46ae-8903-5bbc0354103b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
