{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78bd606-dbca-4ac1-bdd0-b2c2a4921f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) Unsafe program that often causes OOM\n",
    "\n",
    "This intentionally creates a large “large_df” and a non-broadcast join with a small-ish lookup_df — plus a .collect() \n",
    "at the end to force driver OOM. On a modest cluster this will either explode executors (shuffle OOM) or the driver (collect OOM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18226b-343b-48f5-b557-bd1532b6330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_program.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OOMDemoBad\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Simulate a large dataset (replace range size to exceed your memory)\n",
    "large_df = spark.range(0, 50_000_000).withColumnRenamed(\"id\", \"user_id\")  # 50M rows\n",
    "\n",
    "# Small lookup but still possibly > broadcast threshold in some environments\n",
    "lookup_data = [(i, f\"category_{i%100}\") for i in range(100000)]  # 100k rows\n",
    "lookup_df = spark.createDataFrame(lookup_data, [\"user_id\", \"category\"])\n",
    "\n",
    "# This join will cause a large shuffle (SortMergeJoin) and possibly OOM\n",
    "joined = large_df.join(lookup_df, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "# Force everything to the driver -> driver OOM for large outputs\n",
    "result = joined.collect()   # DANGEROUS: collects entire joined result to driver\n",
    "\n",
    "print(\"Rows:\", len(result))\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690c53e-2381-4d90-bfaa-154f939354e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Fix A — Broadcast the small table (best when one side is small)\n",
    "# fix_broadcast.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark = SparkSession.builder.appName(\"OOMFixBroadcast\").getOrCreate()\n",
    "\n",
    "large_df = spark.range(0, 50_000_000).withColumnRenamed(\"id\", \"user_id\")\n",
    "lookup_df = spark.read.parquet(\"/path/to/lookup_small.parquet\")  # or as created earlier\n",
    "\n",
    "# Broadcast the small DataFrame\n",
    "joined = large_df.join(broadcast(lookup_df), on=\"user_id\", how=\"inner\")\n",
    "\n",
    "# Do NOT collect results; write to disk instead\n",
    "joined.write.mode(\"overwrite\").parquet(\"/tmp/joined_output/\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a96f1b-4aed-4355-a751-ce40f154cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "3) Fix B — Repartition to increase parallelism and reduce partition size\n",
    "# fix_repartition.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"OOMFixRepartition\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 400)   # increase shuffle parallelism\n",
    "\n",
    "large_df = spark.range(0, 50_000_000).withColumnRenamed(\"id\", \"user_id\")\n",
    "\n",
    "# Repartition large_df before join so per-partition memory is lower\n",
    "large_df = large_df.repartition(400, \"user_id\")\n",
    "\n",
    "lookup_df = spark.createDataFrame([(i, f\"category_{i%100}\") for i in range(100000)],\n",
    "                                  [\"user_id\", \"category\"])\n",
    "\n",
    "joined = large_df.join(lookup_df, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "# Write out rather than collect\n",
    "joined.write.mode(\"overwrite\").parquet(\"/tmp/joined_output_repart/\")\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01506a-22eb-4c76-aadd-af362b5d4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Fix C — Persist with MEMORY_AND_DISK and unpersist when done\n",
    "# fix_persist.py\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"OOMFixPersist\").getOrCreate()\n",
    "\n",
    "df = spark.range(0, 50_000_000).withColumnRenamed(\"id\", \"user_id\")\n",
    "# some transformations\n",
    "df = df.withColumn(\"flag\", (df.user_id % 2 == 0).cast(\"int\"))\n",
    "\n",
    "# Persist to memory+disk\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Trigger materialization once\n",
    "df.count()\n",
    "\n",
    "# Use df multiple times safely (will spill to disk, avoiding OOM)\n",
    "# ... do joins, aggregations ...\n",
    "df.unpersist()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e2986-ffdc-4f8f-b9e0-f648d328320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "8) How to verify the fix (what to check)\n",
    "\n",
    "Spark UI (http://<driver-host>:4040)\n",
    "\n",
    "Storage tab: cached RDD/DataFrame sizes\n",
    "Stages tab: shuffle read/write sizes; long tasks\n",
    "Executors tab: memory usage, GC time\n",
    "Logs: look for OutOfMemoryError, TaskKilled due to executor loss, or lots of spill to disk messages.\n",
    "Execution plan: df.explain(True) — check for BroadcastHashJoin vs SortMergeJoin.\n",
    "Time/size metrics: successful runs should show lower shuffle/write/read per task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark_kernel",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
