{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88f71a0-069d-43b7-82e3-7ff9375dc93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 21:03:58 WARN Utils: Your hostname, user-HP-Pavilion-x360-Convertible-14-dh0xxx resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface wlo1)\n",
      "25/11/26 21:03:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/26 21:03:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created base DataFrame with 10000000 records and 10 partitions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Initialize Spark Session\n",
    "# Note: For better visibility on the Spark UI, ensure you run this on a cluster \n",
    "# with at least 2 executors/workers.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkUIDebugger\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set logging level for cleaner console output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# --- 1. SETUP: Create a large baseline DataFrame (10 Million Records) ---\n",
    "NUM_RECORDS = 10_000_000\n",
    "\n",
    "# Create an RDD for efficient data generation\n",
    "rdd = spark.sparkContext.parallelize(range(NUM_RECORDS))\n",
    "\n",
    "# Convert RDD to DataFrame with a unique ID and a random group key (1 to 100)\n",
    "base_df = rdd.map(lambda i: (i, random.randint(1, 100), f\"Item_{i}\")) \\\n",
    "             .toDF([\"id\", \"group_key\", \"item_name\"]) \\\n",
    "             .repartition(10) # Ensure data is split across partitions for visualization\n",
    "\n",
    "print(f\"Created base DataFrame with {NUM_RECORDS} records and 10 partitions.\")\n",
    "base_df.cache() # Cache for re-use and to demonstrate the Storage tab\n",
    "base_df.count() # Action to materialize the cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee5f82-0e65-4246-b679-b0de1319b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# SCENARIO A: High Shuffle & Wide Transformation (Jobs and Stages Tab)\n",
    "# Goal: Observe a large \"Shuffle Write\" and \"Shuffle Read\" step.\n",
    "# ======================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28289b-2c17-4f15-8fde-60e9efd7a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Running Scenario A: High Shuffle Aggregation ---\")\n",
    "print(\"Spark UI Tip: Look at the SQL/Graph tab for a large 'Exchange' node.\")\n",
    "\n",
    "# This operation requires a full redistribution of data across the network (Shuffle)\n",
    "# to group all matching 'group_key' records together for the aggregation.\n",
    "df_shuffled = base_df.groupBy(\"group_key\").agg(\n",
    "    F.count(\"id\").alias(\"total_count\"),\n",
    "    F.max(\"id\").alias(\"max_id\")\n",
    ")\n",
    "\n",
    "# Trigger the action and measure the time\n",
    "start_time_a = time.time()\n",
    "df_shuffled.collect() # Use collect() to force computation to the driver\n",
    "end_time_a = time.time()\n",
    "\n",
    "print(f\"Scenario A completed in {end_time_a - start_time_a:.2f} seconds.\")\n",
    "print(\"In the Spark UI: Go to the 'Stages' tab for the latest Job. Look for the 'Shuffle Read' and 'Shuffle Write' metrics. They should be significantly high.\")\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7807d9-5386-4a2c-b297-c49d38687962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# SCENARIO B: Data Skew (Stages Tab: Stragglers)\n",
    "# Goal: Artificially create a straggler task where one task processes 99% of data.\n",
    "# =======================================================================\n",
    "\n",
    "print(\"\\n--- Running Scenario B: Data Skew ---\")\n",
    "print(\"Spark UI Tip: Look for one task taking significantly longer than others in the 'Stages' tab.\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "# Create skewed data: 99% of records get 'group_key' = 9999\n",
    "skewed_rdd = spark.sparkContext.parallelize(range(NUM_RECORDS))\n",
    "skewed_df = skewed_rdd.map(lambda i: (i, 9999 if i < NUM_RECORDS * 0.99 else random.randint(1, 10), f\"SkewItem_{i}\")) \\\n",
    "                      .toDF([\"id\", \"skew_key\", \"item_name\"])\n",
    "\n",
    "# The aggregation will force a shuffle on 'skew_key'. \n",
    "# Since almost all data belongs to key '9999', one partition's task \n",
    "# will be massive (the straggler).\n",
    "df_skewed_agg = skewed_df.groupBy(\"skew_key\").agg(F.count(\"id\").alias(\"count\"))\n",
    "\n",
    "# Trigger the action\n",
    "start_time_b = time.time()\n",
    "df_skewed_agg.collect() \n",
    "end_time_b = time.time()\n",
    "\n",
    "print(f\"Scenario B completed in {end_time_b - start_time_b:.2f} seconds.\")\n",
    "print(\"In the Spark UI: Go to the 'Stages' tab. Locate the stage with the aggregation. You should see Task metrics where the 'Duration' of one task is far greater than the others, and its 'Input Size' is much larger (Data Skew).\")\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3dcad-b637-4d8b-b9e7-16fef40dbe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col\n",
    "import random\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Disable AQE\n",
    "# -------------------------------\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"AQE_Off_Skew_Test\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"false\")  # Turn off AQE\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"false\")\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")   # Helps visualize skew\n",
    "print(\"AQE Enabled?:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Create skewed DataFrame\n",
    "# -------------------------------\n",
    "\n",
    "# Many rows with key=1 (majority) to create skew\n",
    "skewed_data = []\n",
    "for i in range(200000):  # 2 million rows\n",
    "    if i < 180000:\n",
    "        skewed_data.append((1, random.randint(1, 100)))\n",
    "    else:\n",
    "        skewed_data.append((i % 1000 + 2, random.randint(1, 100)))\n",
    "\n",
    "df_skewed = spark.createDataFrame(skewed_data, [\"id\", \"value\"])\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Create a small lookup table\n",
    "# -------------------------------\n",
    "\n",
    "lookup_data = [(i, f\"name_{i}\") for i in range(1, 500)]\n",
    "df_lookup = spark.createDataFrame(lookup_data, [\"id\", \"name\"])\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Trigger skew with a join\n",
    "# -------------------------------\n",
    "\n",
    "joined_df = df_skewed.join(df_lookup, \"id\", \"inner\")\n",
    "\n",
    "print(\"Starting skewed join...\")\n",
    "joined_df.count()   # ACTION to force execution\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Stop session\n",
    "# -------------------------------\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106f0a32-8a4d-4488-9892-6b14630a2868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Scenario C: UDF Bottleneck ---\n",
      "Spark UI Tip: Look at the 'Summary Metrics' in the Stages tab for high 'Executor CPU Time'.\n",
      "Scenario C completed in 0.48 seconds.\n",
      "In the Spark UI: Go to the 'Stages' tab. This stage should show a high value for 'Executor CPU Time' and low 'Shuffle Read/Write', indicating the bottleneck is in computation, not networking.\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# SCENARIO C: UDF Bottleneck (Stages Tab: Task CPU Time)\n",
    "# Goal: Simulate a slow User Defined Function (UDF) that dominates CPU time.\n",
    "# =======================================================================\n",
    "\n",
    "print(\"\\n--- Running Scenario C: UDF Bottleneck ---\")\n",
    "print(\"Spark UI Tip: Look at the 'Summary Metrics' in the Stages tab for high 'Executor CPU Time'.\")\n",
    "\n",
    "# 1. Define a very slow UDF that sleeps (simulates complex, non-optimized logic)\n",
    "def slow_transform(val):\n",
    "    \"\"\"Simulates a slow, expensive operation.\"\"\"\n",
    "    time.sleep(0.0001) # Small sleep, but multiplies by 10 million rows\n",
    "    return val.upper()\n",
    "\n",
    "slow_udf = F.udf(slow_transform, StringType())\n",
    "\n",
    "# 2. Apply the slow UDF to the large DataFrame (requires a local Python context execution)\n",
    "df_slow_udf = base_df.withColumn(\"transformed_name\", slow_udf(F.col(\"item_name\")))\n",
    "\n",
    "# 3. Trigger the action\n",
    "start_time_c = time.time()\n",
    "df_slow_udf.count() # Count forces execution\n",
    "end_time_c = time.time()\n",
    "\n",
    "print(f\"Scenario C completed in {end_time_c - start_time_c:.2f} seconds.\")\n",
    "print(\"In the Spark UI: Go to the 'Stages' tab. This stage should show a high value for 'Executor CPU Time' and low 'Shuffle Read/Write', indicating the bottleneck is in computation, not networking.\")\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5abcb646-476d-481d-ac19-74020ef120c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import threading\n",
    "from datetime import datetime\n",
    "\n",
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Stores all metrics\n",
    "job_metrics = {}\n",
    "stage_metrics = {}\n",
    "task_metrics = {}\n",
    "\n",
    "\n",
    "class FullMetricsListener:\n",
    "    def __init__(self):\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    # --------------------------\n",
    "    #   JOB METRICS\n",
    "    # --------------------------\n",
    "    def onJobStart(self, jobStart):\n",
    "        with self.lock:\n",
    "            job_id = jobStart.jobId()\n",
    "            job_metrics[job_id] = {\n",
    "                \"job_id\": job_id,\n",
    "                \"status\": \"RUNNING\",\n",
    "                \"stage_ids\": list(jobStart.stageIds()),\n",
    "                \"start_time\": datetime.now(),\n",
    "                \"end_time\": None,\n",
    "                \"duration_ms\": None\n",
    "            }\n",
    "\n",
    "    def onJobEnd(self, jobEnd):\n",
    "        with self.lock:\n",
    "            job_id = jobEnd.jobId()\n",
    "            status = (\n",
    "                \"SUCCEEDED\" if \"JobSucceeded\" in str(jobEnd.jobResult()) \n",
    "                else \"FAILED\"\n",
    "            )\n",
    "\n",
    "            job_metrics[job_id][\"status\"] = status\n",
    "            job_metrics[job_id][\"end_time\"] = datetime.now()\n",
    "\n",
    "            start = job_metrics[job_id][\"start_time\"]\n",
    "            end = job_metrics[job_id][\"end_time\"]\n",
    "            job_metrics[job_id][\"duration_ms\"] = (end - start).total_seconds() * 1000\n",
    "\n",
    "    # --------------------------\n",
    "    #   STAGE METRICS\n",
    "    # --------------------------\n",
    "    def onStageSubmitted(self, stageSubmitted):\n",
    "        with self.lock:\n",
    "            info = stageSubmitted.stageInfo()\n",
    "            stage_id = info.stageId()\n",
    "\n",
    "            stage_metrics[stage_id] = {\n",
    "                \"stage_id\": stage_id,\n",
    "                \"name\": info.name(),\n",
    "                \"status\": \"RUNNING\",\n",
    "                \"num_tasks\": info.numTasks(),\n",
    "                \"start_time\": datetime.now(),\n",
    "                \"end_time\": None,\n",
    "                \"duration_ms\": None,\n",
    "                \"shuffle_read_bytes\": 0,\n",
    "                \"shuffle_write_bytes\": 0,\n",
    "                \"input_bytes\": 0,\n",
    "                \"output_bytes\": 0,\n",
    "                \"executor_run_time_ms\": 0,\n",
    "                \"gc_time_ms\": 0\n",
    "            }\n",
    "\n",
    "    def onStageCompleted(self, stageCompleted):\n",
    "        with self.lock:\n",
    "            info = stageCompleted.stageInfo()\n",
    "            stage_id = info.stageId()\n",
    "\n",
    "            metrics = info.taskMetrics()\n",
    "\n",
    "            stage_metrics[stage_id][\"status\"] = \"SUCCEEDED\"\n",
    "            stage_metrics[stage_id][\"end_time\"] = datetime.now()\n",
    "\n",
    "            start = stage_metrics[stage_id][\"start_time\"]\n",
    "            end = stage_metrics[stage_id][\"end_time\"]\n",
    "\n",
    "            stage_metrics[stage_id][\"duration_ms\"] = (end - start).total_seconds() * 1000\n",
    "\n",
    "            # Metrics\n",
    "            stage_metrics[stage_id][\"shuffle_read_bytes\"] = metrics.shuffleReadMetrics().totalBytesRead()\n",
    "            stage_metrics[stage_id][\"shuffle_write_bytes\"] = metrics.shuffleWriteMetrics().bytesWritten()\n",
    "            stage_metrics[stage_id][\"input_bytes\"] = metrics.inputMetrics().bytesRead()\n",
    "            stage_metrics[stage_id][\"output_bytes\"] = metrics.outputMetrics().bytesWritten()\n",
    "            stage_metrics[stage_id][\"executor_run_time_ms\"] = metrics.executorRunTime()\n",
    "            stage_metrics[stage_id][\"gc_time_ms\"] = metrics.jvmGCTime()\n",
    "\n",
    "    # --------------------------\n",
    "    #   TASK METRICS\n",
    "    # --------------------------\n",
    "    def onTaskEnd(self, taskEnd):\n",
    "        with self.lock:\n",
    "            info = taskEnd.taskInfo()\n",
    "            metrics = taskEnd.taskMetrics()\n",
    "\n",
    "            task_id = info.taskId()\n",
    "\n",
    "            task_metrics[task_id] = {\n",
    "                \"task_id\": task_id,\n",
    "                \"stage_id\": info.stageId(),\n",
    "                \"host\": info.host(),\n",
    "                \"executor_id\": info.executorId(),\n",
    "                \"status\": \"FINISHED\",\n",
    "                \"duration_ms\": info.duration(),\n",
    "                \"input_bytes\": metrics.inputMetrics().bytesRead(),\n",
    "                \"output_bytes\": metrics.outputMetrics().bytesWritten(),\n",
    "                \"shuffle_read_bytes\": metrics.shuffleReadMetrics().totalBytesRead(),\n",
    "                \"shuffle_write_bytes\": metrics.shuffleWriteMetrics().bytesWritten(),\n",
    "                \"executor_cpu_time_ms\": metrics.executorCpuTime() / 1e6,\n",
    "                \"executor_run_time_ms\": metrics.executorRunTime(),\n",
    "                \"gc_time_ms\": metrics.jvmGCTime()\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2df9e4-85b2-45e8-9016-ea29e32cdd0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m java_import(gateway\u001b[38;5;241m.\u001b[39mjvm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m listener \u001b[38;5;241m=\u001b[39m FullMetricsListener()\n\u001b[0;32m----> 7\u001b[0m jlistener \u001b[38;5;241m=\u001b[39m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonListener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlistener\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m sc\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39maddSparkListener(jlistener)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "gateway = sc._gateway\n",
    "java_import(gateway.jvm, \"org.apache.spark.*\")\n",
    "\n",
    "listener = FullMetricsListener()\n",
    "jlistener = gateway.jvm.PythonListener(listener)\n",
    "\n",
    "sc._jsc.sc().addSparkListener(jlistener)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f2f08-d24d-486d-a7b7-d1b4e667189e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
