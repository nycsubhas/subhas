{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45dde89b-169a-4109-ac76-43ccf3683245",
   "metadata": {},
   "source": [
    "* I have a Azure Eventhub, which is streaming data (in JSON format). I read it as a Spark dataframe, parse the incoming \"body\" with from_json(col(\"body\"), schema) where schema is pre-defined. In code it, looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2671fd5f-8e1c-4b76-b421-fec282478540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source - https://stackoverflow.com/q\n",
    "# Posted by mLC, modified by community. See post 'Timeline' for change history\n",
    "# Retrieved 2025-11-16, License - CC BY-SA 4.0\n",
    "\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType().add(...) # define the incoming JSON schema \n",
    "\n",
    "df_stream_input = (spark\n",
    ".readStream\n",
    ".format(\"eventhubs\")\n",
    ".options(**ehConfInput)\n",
    ".load()\n",
    ".select(from_json(col(\"body\").cast(\"string\"), schema)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2104d388-8905-486a-b1d1-9bed2be33314",
   "metadata": {},
   "source": [
    "And now = if there is some inconsistency between the incoming JSON's schema and the defined schema (e.g. the source eventhub starts sending data in new format without notice), the from_json() functions will not throw an error = instead, it will put NULL to the fields, which are present in my schema definition but not in the JSONs eventhub sends.\n",
    "\n",
    "I want to capture this information and log it somewhere (Spark's log4j, Azure Monitor, warning email, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2c270-2e6f-4755-b023-bbd18931870c",
   "metadata": {},
   "source": [
    "**My question is: what is the best way how to achieve this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e37aaf-e5e8-4955-a613-a6f7aaaa18e6",
   "metadata": {},
   "source": [
    "Some of my thoughts:\n",
    "\n",
    "* First thing I can think of is to have a UDF, which checks for the NULLs and if there is any problem, it raise an Exception. I believe there it is not possible to send logs to log4j via PySpark, as the \"spark\" context cannot be initiated within the UDF (on the workers) and one wants to use the default:\n",
    "\n",
    "* log4jLogger = sc._jvm.org.apache.log4j logger = log4jLogger.LogManager.getLogger('PySpark Logger')\n",
    "\n",
    "* Second thing I can think of is to use \"foreach/foreachBatch\" function and put this check logic there.\n",
    "\n",
    "* But I feel both these approaches are like.. like too much custom - I was hoping that Spark has something built-in for these purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff76fb-415d-415d-afb0-61a3fd7fa356",
   "metadata": {},
   "outputs": [],
   "source": [
    "It turns out I was mistaken thinking that columnNameOfCorruptRecord option could be an answer. It will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3bfc0-d5e7-4ee1-bf7d-8e52aa417399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source - https://stackoverflow.com/a\n",
    "# Posted by Jacek Laskowski, modified by community. See post 'Timeline' for change history\n",
    "# Retrieved 2025-11-16, License - CC BY-SA 4.0\n",
    "\n",
    "case _: BadRecordException => null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0354591-c28f-49b2-82cd-ecd61725903d",
   "metadata": {},
   "source": [
    "* And secondly due to this that simply disables any other parsing modes (incl. PERMISSIVE that seems to be used alongside columnNameOfCorruptRecord option):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc5849-7566-4f0b-8bd0-9e53b87ddcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source - https://stackoverflow.com/a\n",
    "# Posted by Jacek Laskowski, modified by community. See post 'Timeline' for change history\n",
    "# Retrieved 2025-11-16, License - CC BY-SA 4.0\n",
    "\n",
    "new JSONOptions(options + (\"mode\" -> FailFastMode.name), timeZoneId.get))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58413b8-dc47-438f-81e8-714ab1333a29",
   "metadata": {},
   "source": [
    "* In other words, your only option is to use the 2nd item in your list, i.e. foreach or foreachBatch and handle corrupted records yourself.\n",
    "\n",
    "* A solution could use from_json while keeping the initial body column. Any record with an incorrect JSON would end up with the result column null and foreach* would catch it, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c2f6d-3271-4664-8bf6-13e83019f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source - https://stackoverflow.com/a\n",
    "# Posted by Jacek Laskowski, modified by community. See post 'Timeline' for change history\n",
    "# Retrieved 2025-11-16, License - CC BY-SA 4.0\n",
    "\n",
    "def handleCorruptRecords:\n",
    "  // if json == null the body was corrupt\n",
    "  // handle it\n",
    "\n",
    "df_stream_input = (spark\n",
    "  .readStream\n",
    "  .format(\"eventhubs\")\n",
    "  .options(**ehConfInput)\n",
    "  .load()\n",
    "  .select(\"body\", from_json(col(\"body\").cast(\"string\"), schema).as(\"json\"))\n",
    ").foreach(handleCorruptRecords).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4acea00-25f5-4828-a01b-f752d33f0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "def handleCorruptRecords(df, batch_id):\n",
    "    \"\"\"\n",
    "    This function runs for every micro-batch in the stream.\n",
    "    df        : dataframe for that batch\n",
    "    batch_id  : micro-batch ID\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Processing batch: {batch_id}\")\n",
    "\n",
    "    # rows where JSON parsed correctly\n",
    "    good_records = df.filter(col(\"json\").isNotNull())\n",
    "\n",
    "    # malformed rows - JSON failed to parse\n",
    "    corrupt_records = df.filter(col(\"json\").isNull())\n",
    "\n",
    "    # Show counts (or write to Delta)\n",
    "    print(f\"Good Records: {good_records.count()}\")\n",
    "    print(f\"Corrupt Records: {corrupt_records.count()}\")\n",
    "\n",
    "    # Example: write good records to Silver\n",
    "    good_records.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(\"/mnt/silver/good_records\")\n",
    "\n",
    "    # Example: write bad records to a quarantine location\n",
    "    corrupt_records.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(\"/mnt/bronze/corrupt_records\")\n",
    "\n",
    "    # You can also log, alert, etc.\n",
    "\n",
    "\n",
    "# EVENT HUB READER\n",
    "df_stream_input = (\n",
    "    spark.readStream\n",
    "        .format(\"eventhubs\")\n",
    "        .options(**ehConfInput)\n",
    "        .load()\n",
    "        .select(\n",
    "            \"body\",\n",
    "            from_json(col(\"body\").cast(\"string\"), schema).alias(\"json\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# APPLY foreachBatch\n",
    "query = (\n",
    "    df_stream_input.writeStream\n",
    "        .foreachBatch(handleCorruptRecords)\n",
    "        .option(\"checkpointLocation\", \"/mnt/checkpoints/eh_json_stream\")\n",
    "        .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590cbce-56fc-4088-a87f-520fcfbc1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Why json == null means corrupt?**\n",
    "If from_json() cannot parse the JSON:\n",
    "\n",
    "from_json(col(\"body\").cast(\"string\"), schema) = null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff33fe-89ae-4bc6-9201-bf93abe73d2e",
   "metadata": {},
   "source": [
    "So:\n",
    "\n",
    "* json.isNotNull() → valid JSON\n",
    "\n",
    "* json.isNull() → malformed/corrupt JSON\n",
    "\n",
    "* This is the standard approach in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd52262-7c37-4827-b8e1-e8adcc064747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import ForeachWriter\n",
    "\n",
    "class CorruptRecordWriter(ForeachWriter):\n",
    "\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        # Called once for every partition and epoch\n",
    "        # Return True to continue\n",
    "        print(f\"Opening writer for partition: {partition_id}, epoch: {epoch_id}\")\n",
    "        return True      \n",
    "\n",
    "    def process(self, row):\n",
    "        # row.json will be None if JSON parsing failed\n",
    "        if row.json is None:\n",
    "            print(f\"CORRUPT RECORD FOUND: {row.body_string}\")\n",
    "            # Here you can store into quarantine table.\n",
    "            # spark.sql(\"INSERT INTO delta.`/mnt/silver/quarantine` VALUES (...)\")\n",
    "        else:\n",
    "            print(f\"VALID RECORD: {row.json}\")\n",
    "            # Write valid JSON to your silver table\n",
    "            # spark.sql(\"INSERT INTO delta.`/mnt/silver/clean` VALUES (...)\")\n",
    "\n",
    "    def close(self, error):\n",
    "        if error:\n",
    "            print(f\"Error occurred: {error}\")\n",
    "        else:\n",
    "            print(\"Completed processing partition.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1358f3d-c239-4221-b31d-9ab8dd6ba467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "\n",
    "schema = ...  # your JSON schema\n",
    "\n",
    "df_stream_input = (\n",
    "    spark.readStream\n",
    "        .format(\"eventhubs\")\n",
    "        .options(**ehConfInput)\n",
    "        .load()\n",
    "        .select(\n",
    "            col(\"body\").cast(\"string\").alias(\"body_string\"),\n",
    "            from_json(col(\"body\").cast(\"string\"), schema).alias(\"json\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Apply ForeachWriter\n",
    "query = (\n",
    "    df_stream_input.writeStream\n",
    "        .foreach(CorruptRecordWriter())\n",
    "        .outputMode(\"append\")\n",
    "        .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9775c-846c-4e76-a92a-9ba254e8b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "How It Works\n",
    "✔ Valid JSON\n",
    "\n",
    "from_json() parses the JSON → struct is created → row.json is not null\n",
    "\n",
    "✔ Corrupt JSON\n",
    "\n",
    "Parsing fails → returns None (NULL) → handled inside process(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
