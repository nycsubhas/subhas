{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147b83c-e5ab-496e-b73a-9ca014877461",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Basic Databricks Questions\n",
    "\n",
    "\n",
    "What is Databricks?\n",
    "Databricks is a unified data analytics platform built on Apache Spark that supports data engineering, \n",
    "data science, machine learning, and BI tasks via a Lakehouse architecture.\n",
    "\n",
    "\n",
    "What is a cluster in Databricks? What are the different cluster types?\n",
    "A cluster is a set of compute (VMs) that run your Spark workloads. \n",
    "Types include all-purpose (interactive) and job clusters (ephemeral for jobs).\n",
    "\n",
    "\n",
    "Difference between a job cluster and an all-purpose cluster?\n",
    "\n",
    "All-purpose: multi-user, shared, interactive development\n",
    "\n",
    "Job cluster: spun up per job, more transient and optimized for scheduled workloads\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is the Databricks Workspace?\n",
    "The workspace is the UI environment where users create and manage notebooks, jobs, folders, and repos.\n",
    "\n",
    "\n",
    "What are notebooks in Databricks and which languages are supported?\n",
    "Notebooks are interactive documents for code, visualizations, and markdown. \n",
    "Supported languages include Python, SQL, Scala, R, and also SQL + Python mixed via %sql, %python etc.\n",
    "\n",
    "\n",
    "What is DBFS (Databricks File System)?\n",
    "DBFS is a distributed file system mounted into Databricks clusters, abstracting storage (like S3, ADLS) into a file-like API.\n",
    "\n",
    "\n",
    "How do you mount Azure Data Lake or AWS S3 in Databricks?\n",
    "Use DBFS mounting commands (e.g., dbutils.fs.mount) or \n",
    "configure access via IAM roles / service principals and \n",
    "use direct access via path URIs.\n",
    "\n",
    "\n",
    "What is Auto Loader?\n",
    "Auto Loader is a Databricks feature that incrementally ingests new data files from cloud storage \n",
    "(e.g., S3 or ADLS) using a schema-aware file notification mechanism.\n",
    "\n",
    "\n",
    "What are widgets in Databricks notebooks?\n",
    "Widgets allow parameterization of notebooks (dropdowns, text boxes) so that jobs can be run with \n",
    "different input parameters.\n",
    "\n",
    "\n",
    "How do you schedule a Databricks notebook as a job?\n",
    "Use the Databricks Jobs UI or API, define a job with notebook, set cluster spec, schedule (cron), \n",
    "and optionally dependent tasks.\n",
    "\n",
    "   Method 1: Using the Jobs UI (Recommended)\n",
    "In the left sidebar, click Jobs.\n",
    "Click Create job.\n",
    "In the Tasks tab, click Add task.\n",
    "Give the task a name and select Notebook from the Type drop-down menu.\n",
    "Under Source, select Workspace and browse to find your notebook.\n",
    "Configure a job cluster or select an existing cluster for the task.\n",
    "Optionally, set up a schedule, alerts, or parameters and click Create. \n",
    "\n",
    "    Method 2: Scheduling from the notebook\n",
    "Open the notebook you want to run as a job.\n",
    "Click the Schedule button in the top-right corner.\n",
    "If jobs already exist for the notebook, click Add a schedule.\n",
    "In the Schedule dialog, enter a name, choose a schedule type (Simple or Advanced), and select the compute resource.\n",
    "You can also add parameters or email alerts to be sent on job events.\n",
    "Click Create (or Submit) to save the scheduled job. \n",
    "\n",
    "\n",
    "2. Intermediate Databricks / Delta Questions\n",
    "\n",
    "\n",
    "Explain the advantages of Delta Lake.\n",
    "Delta Lake brings ACID transactions, time-travel, scalable metadata handling, and schema enforcement \n",
    "to data lakes.\n",
    "\n",
    "\n",
    "What problem does Delta Lake solve?\n",
    "It solves data reliability, consistency, and performance issues in data lakes (e.g., handling concurrent writes, data corruption, small files).\n",
    "\n",
    "\n",
    "Delta Lake features: ACID, time travel, schema enforcement, schema evolution.\n",
    "\n",
    "\n",
    "ACID: ensures transactional consistency\n",
    "\n",
    "\n",
    "Time travel: query data as of previous versions\n",
    "\n",
    "\n",
    "Schema enforcement: reject writes not matching schema\n",
    "\n",
    "\n",
    "Schema evolution: allows schema to change over time in a controlled way\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How does schema enforcement work in Delta Lake?\n",
    "When writing to a Delta table, Delta validates incoming data against the defined schema and \n",
    "rejects the write if incompatible.\n",
    "\n",
    "\n",
    "Difference between MERGE and UPDATE in Delta?\n",
    "\n",
    "\n",
    "UPDATE: modifies existing rows based on a condition\n",
    "\n",
    "\n",
    "MERGE: upsert pattern — can insert, update, or delete based on matching conditions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is OPTIMIZE and ZORDER in Databricks?\n",
    "\n",
    "\n",
    "OPTIMIZE: compacts small files into larger ones for performance\n",
    "\n",
    "\n",
    "ZORDER: reorders data within files by specified columns, improving query performance (especially for range queries)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is the small files problem in Delta tables?\n",
    "When many tiny files are created (e.g., from micro-batches), query performance suffers due to overhead. \n",
    "It can be mitigated by compaction (OPTIMIZE).\n",
    "\n",
    "\n",
    "What is the purpose of VACUUM in Delta?\n",
    "VACUUM removes old, unneeded files from Delta’s storage, freeing up space but you must be careful because it deletes historical data \n",
    "beyond retention.\n",
    "\n",
    "\n",
    "Explain COPY INTO. When do you use it?\n",
    "COPY INTO is used to bulk load data into a Delta table from cloud storage, especially when there are many files or partitioned data.\n",
    "\n",
    "\n",
    "What is Databricks Runtime (DBR)?\n",
    "DBR is the managed Spark runtime provided by Databricks, optimized for performance, with pre-installed libraries, ML support, GPU support, and more.\n",
    "\n",
    "\n",
    "\n",
    "3. Spark / Databricks SQL Questions\n",
    "\n",
    "\n",
    "Difference between Spark DataFrame and Pandas DataFrame.\n",
    "\n",
    "\n",
    "Spark DataFrame: distributed, parallel, fault-tolerant\n",
    "\n",
    "\n",
    "Pandas DataFrame: in-memory, single-node, good for small-to-medium data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is the Catalyst Optimizer and Tungsten?\n",
    "\n",
    "\n",
    "Catalyst: Spark’s query optimizer, does logical and physical planning\n",
    "\n",
    "\n",
    "Tungsten: Spark’s execution engine optimizations (memory, code gen) for speed and efficiency\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Explain narrow vs wide transformations in Spark.\n",
    "\n",
    "\n",
    "Narrow: partitions don’t need data from other partitions (e.g., map, filter)\n",
    "\n",
    "\n",
    "Wide: require shuffles / data exchange across partitions (e.g., groupBy, join)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How do you handle skew in Spark?\n",
    "\n",
    "\n",
    "1. Broadcast small tables for joins\n",
    "\n",
    "2. Salting keys to distribute skewed keys\n",
    "\n",
    "3. Using adaptive query execution (AQE)\n",
    "\n",
    "4. Repartition data manually\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What causes shuffle in Spark?\n",
    "Shuffle happens when data needs to be redistributed across partitions — e.g., \n",
    "groupBy, join, distinct, repartition.\n",
    "\n",
    "\n",
    "How do you reduce shuffle in Spark applications?\n",
    "\n",
    "\n",
    "1. Use broadcast joins.\n",
    "2. Use map-side aggregations.\n",
    "3. Use partitioning.\n",
    "4. Use coalesce properly.\n",
    "5. Use filter, select early to reduce data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is broadcast join? When should you use it?\n",
    "A broadcast join sends a small dataset to all executors so a large dataset can join locally without \n",
    "shuffling. Use when one side of the join is small.\n",
    "\n",
    "\n",
    "What is AQE (Adaptive Query Execution)?\n",
    "Adaptive Query Execution dynamically adjusts the query plan at runtime, e.g., \n",
    "changing join strategy or number of shuffle partitions based on statistics.\n",
    "\n",
    "\n",
    "What are Spark UDF, Pandas UDF, and SQL functions?\n",
    "\n",
    "Spark UDF: user-defined function using JVM (Scala/Java/Python), slower\n",
    "Pandas UDF (vectorized): uses Apache Arrow, faster, good for batch vectorized operations\n",
    "SQL functions: built-in Spark SQL functions, most efficient\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is checkpointing vs. caching in Spark?\n",
    "Caching: storing data in memory/disk to reuse it across operations\n",
    "Checkpointing: writing to stable storage (HDFS/DBFS) to truncate lineage and provide fault tolerance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. Lakehouse & Architecture Questions\n",
    "\n",
    "What is the Databricks Lakehouse Platform?\n",
    "A unified architecture combining data lake flexibility and data warehouse reliability with Delta Lake \n",
    "as the storage layer.\n",
    "\n",
    "How does the Lakehouse differ from a traditional data warehouse?\n",
    "Lakehouse uses cheaper, scalable object storage\n",
    "Supports batch & streaming\n",
    "Provides ACID via Delta\n",
    "More flexible schema handling\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is Unity Catalog?\n",
    "A centralized data governance service in Databricks that handles access control, \n",
    "metadata management, and lineage across your lakehouse.\n",
    "\n",
    "\n",
    "How do you secure data using Unity Catalog?\n",
    "By defining catalogs, schemas, and tables, and granting fine-grained permissions \n",
    "(SELECT, INSERT, MODIFY) on them; \n",
    "also using data lineage and audit logs.\n",
    "\n",
    "\n",
    "Difference between a catalog, schema, and table in Unity Catalog.\n",
    "Catalog: top-level namespace (e.g., “finance_catalog”)\n",
    "Schema: a namespace inside a catalog (like a database)\n",
    "Table: the actual data structure inside a schema\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is the medallion architecture (Bronze, Silver, Gold)?\n",
    "\n",
    "Bronze: raw, ingested data\n",
    "Silver: cleaned, deduplicated, enriched data\n",
    "Gold: business-level aggregates / curated tables / serving layer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How do you design a Delta Live Tables (DLT) pipeline?\n",
    "\n",
    "\n",
    "0. Define source tables\n",
    "1. Apply transformations (cleaning, business logic)\n",
    "2. Use expectations (data quality)\n",
    "3. Define target tables for Bronze, Silver, Gold\n",
    "4. Set pipeline scheduling and monitoring\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What is serverless SQL warehouse?\n",
    "A fully managed SQL compute resource in Databricks for running BI/analytics queries that scales \n",
    "automatically and you pay per usage.\n",
    "\n",
    "\n",
    "What are DBU charges?\n",
    "DBU = Databricks Unit, a usage-based cost metric representing compute resource consumption in Databricks.\n",
    "\n",
    "\n",
    "Explain cluster autoscaling.\n",
    "Autoscaling automatically adds or removes worker nodes in a cluster based on workload demand to \n",
    "optimize cost and performance.\n",
    "\n",
    "\n",
    "\n",
    "5. Real-Time & Streaming Questions\n",
    "\n",
    "\n",
    "What is Structured Streaming?\n",
    "Spark’s high-level API for stream processing, treating streaming data as incremental micro-batches \n",
    "(or continuous, depending on mode) of DataFrames.\n",
    "\n",
    "\n",
    "What is Auto Loader and how is it different from readStream?\n",
    "Auto Loader is optimized for file-based incremental ingestion; \n",
    "readStream is Spark’s API for reading streaming data regardless of source, but requires more manual \n",
    "handling.\n",
    "\n",
    "\n",
    "What are checkpoints and triggers in streaming?\n",
    "\n",
    "Checkpointing: storing progress and offsets to recover from failures\n",
    "Triggers: define when micro-batches should be processed (e.g., processingTime, once, continuous)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How do you perform stream-to-stream and stream-to-batch join?\n",
    "Stream-to-stream: both sides are streaming DataFrames, careful with watermarking\n",
    "Stream-to-batch: join streaming DataFrame with a static (batch) DataFrame, simpler and common for lookups\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How do you guarantee exactly-once processing in Databricks?\n",
    "Use checkpointing, idempotent writes (e.g., to Delta with merge), and transactional writes supported by Delta Lake.\n",
    "\n",
    "\n",
    "\n",
    "6. Delta Lake Deep-Dive Questions\n",
    "\n",
    "\n",
    "What is a Delta transaction log? Explain _delta_log.\n",
    "_delta_log is the directory containing JSON / Parquet log files that record every transaction (add, remove) for a Delta table\n",
    "to support atomicity and versioning.\n",
    "\n",
    "\n",
    "What is an OPTIMIZE with ZORDER?\n",
    "OPTIMIZE compacts files; ZORDER reorders data within those files on chosen columns, improving query performance for those columns.\n",
    "\n",
    "\n",
    "What is change data feed (CDF) in Delta Lake?\n",
    "CDF allows you to read row-level changes (inserts, updates, deletes) between table versions, which is useful for CDC \n",
    "(change data capture) patterns.\n",
    "\n",
    "\n",
    "How does Delta Lake handle late-arriving data?\n",
    "Use merge logic (e.g., MERGE), watermarking + streaming, or separate staging + deduplication to incorporate late data without corruption.\n",
    "\n",
    "\n",
    "What happens internally when you run a MERGE INTO?\n",
    "Spark reads both source and target, identifies matching keys, generates add and remove files via the Delta transaction log, and writes a new version in an atomic transaction.\n",
    "\n",
    "\n",
    "\n",
    "7. Security & Governance\n",
    "\n",
    "\n",
    "How do you use cluster policies?\n",
    "Define policy rules (e.g., allowed node types, instance pools, autoscaling limits) and enforce them so that users can only spin up clusters that meet compliance/cost guidelines.\n",
    "\n",
    "\n",
    "How does Unity Catalog enforce permission inheritance?\n",
    "Permissions granted at the catalog or schema level propagate down to child objects (tables, views) unless explicitly overridden.\n",
    "\n",
    "\n",
    "How do you audit data access in Databricks?\n",
    "Use Unity Catalog’s lineage and audit logging, or DB’s audit logs (workspace, clusters, jobs) to monitor who accessed what, when, and how.\n",
    "\n",
    "\n",
    "What is Credential Passthrough / IAM roles?\n",
    "\n",
    "\n",
    "Credential Passthrough: forwards user credentials (e.g., via IAM) to the storage layer so that access enforcement happens at the storage level\n",
    "\n",
    "\n",
    "IAM roles: assign roles (e.g., in AWS IAM / Azure RBAC) so that clusters access storage securely using those roles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How do you secure PII data in Databricks?\n",
    "Use Unity Catalog for object-level permissions, implement column-level masking or row-level filtering, encrypt sensitive data, and enforce data governance policies.\n",
    "\n",
    "\n",
    "\n",
    "8. Performance & Optimization\n",
    "\n",
    "\n",
    "How do you handle slow Spark jobs in Databricks?\n",
    "\n",
    "\n",
    "Profile jobs using Spark UI\n",
    "\n",
    "\n",
    "Identify skew, shuffle, or memory spills\n",
    "\n",
    "\n",
    "Use partitioning, caching, or broadcast joins\n",
    "\n",
    "\n",
    "Tune shuffle partitions, executors, or cluster size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How do you identify shuffle spill or skew in the Spark UI?\n",
    "\n",
    "\n",
    "Look at Stage Summary → \"Shuffle Read / Write\" metrics\n",
    "\n",
    "\n",
    "Monitor “Task Deserialization Time” or “Shuffle Spill” in Task View\n",
    "\n",
    "\n",
    "Use the DAG and SQL physical plan to spot data skew\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Best practices to optimize Delta tables?\n",
    "\n",
    "\n",
    "Periodically OPTIMIZE\n",
    "\n",
    "\n",
    "Use ZORDER on frequently filtered columns\n",
    "\n",
    "\n",
    "Partition appropriately\n",
    "\n",
    "\n",
    "Clean up with VACUUM\n",
    "\n",
    "\n",
    "Avoid too many small files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Describe file compaction strategy in Silver layer.\n",
    "In Silver (cleaned) layer, compact small files periodically (or via automated jobs) so downstream analytics / BI queries run efficiently; often OPTIMIZE + ZORDER.\n",
    "\n",
    "\n",
    "How do you split large Bronze → Silver → Gold loads?\n",
    "\n",
    "\n",
    "Bronze: raw ingestion (Auto Loader or batch)\n",
    "\n",
    "\n",
    "Silver: cleaning, deduplication, data quality checks\n",
    "\n",
    "\n",
    "Gold: aggregations, business logic, serving layer\n",
    "Use DLT or scheduled jobs to orchestrate these layers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "9. ML & GenAI (Databricks)\n",
    "\n",
    "\n",
    "What is MLflow?\n",
    "MLflow is an open-source platform for managing the ML lifecycle: tracking experiments, packaging code, deploying models, and registering them.\n",
    "\n",
    "\n",
    "What is Model Serving in Databricks?\n",
    "You can deploy MLflow models as real-time REST API endpoints (or batch), enabling integration with production systems.\n",
    "\n",
    "\n",
    "What is Vector Search in Databricks?\n",
    "Vector Search allows you to store and query embeddings (vectors) efficiently – useful for semantic search, recommendation, and GenAI.\n",
    "\n",
    "\n",
    "What is DBRX?\n",
    "(If by DBRX you mean Databricks’ vector or retrieval-augmented functionality) — DBRX is Databricks’ framework for building applications using retrieval + LLMs, often used for production GenAI.\n",
    "\n",
    "\n",
    "Explain Feature Store in Databricks.\n",
    "A managed store for ML features: centralizes feature definitions, ensures consistency between training and inference, supports batch and streaming feature pipelines, and versioning.\n",
    "\n",
    "\n",
    "\n",
    "10. Scenario-Based Questions\n",
    "\n",
    "\n",
    "Your Delta table has lots of small files. How do you fix it?\n",
    "Use OPTIMIZE to compact the files. Optionally ZORDER by commonly filtered columns. Also adjust upstream batching strategy (larger micro-batches or coalescing).\n",
    "\n",
    "\n",
    "A MERGE operation is running slow. What steps do you take?\n",
    "\n",
    "\n",
    "Check if source or target is skewed\n",
    "\n",
    "\n",
    "Ensure predicates are selective\n",
    "\n",
    "\n",
    "Use broadcast join if appropriate\n",
    "\n",
    "\n",
    "Partitioning strategy\n",
    "\n",
    "\n",
    "Possibly reorganize or optimize the target table\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Your Silver table is showing corrupted records. How do you handle them?\n",
    "\n",
    "\n",
    "Use Delta “expectations” in DLT or validate schema\n",
    "\n",
    "\n",
    "Define quality checks (null checks, regex, ranges)\n",
    "\n",
    "\n",
    "Route bad data to a separate “quarantine” table for later investigation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Your stream stopped due to schema mismatch. How do you fix it?\n",
    "\n",
    "\n",
    "Use schema evolution (Delta)\n",
    "\n",
    "\n",
    "Apply schema enforcement\n",
    "\n",
    "\n",
    "Use Auto Loader with cloudFiles.schemaLocation to track schema changes\n",
    "\n",
    "\n",
    "Validate incoming schema before ingest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How do you enforce data quality rules in Databricks?\n",
    "\n",
    "\n",
    "Use Delta Live Tables (DLT) expectations\n",
    "\n",
    "\n",
    "Implement custom validation in notebooks or jobs\n",
    "\n",
    "\n",
    "Use monitoring, alerting on failed records\n",
    "\n",
    "\n",
    "Create data contracts with producers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "11. Behavioral / HR Questions\n",
    "\n",
    "\n",
    "How did you use Databricks in your last project?\n",
    "Sample answer: “I built a Bronze-Silver-Gold pipeline using Auto Loader, Delta Live Tables, and MLflow. We ingested raw data from S3, cleaned and transformed in Silver, then exposed Gold tables to BI and ML teams.”\n",
    "\n",
    "\n",
    "What part of Databricks do you use the most?\n",
    "“I primarily use Delta Lake and Databricks SQL for building reliable ETL pipelines and serving tables. I also use notebooks for ad hoc data exploration and job orchestration.”\n",
    "\n",
    "\n",
    "What’s the biggest optimization you did in Databricks?\n",
    "“I optimized a job by compacting small files using OPTIMIZE + ZORDER, which reduced query latency by 60%. I also tuned the cluster size and shuffle partitions, which cut costs by 30%.”\n",
    "\n",
    "\n",
    "Explain a time you debugged a slow Spark job.\n",
    "“We had a Spark job with a long shuffle stage. I used the Spark UI to analyze task durations, found data skew on a join key, applied salting to that key, and repartitioned data — performance improved dramatically.”\n",
    "\n",
    "\n",
    "Describe a Databricks architecture you designed.\n",
    "“I designed a lakehouse architecture: raw data from S3 landed in a Bronze Delta table via Auto Loader; then Delta Live Tables cleaned and transformed data into Silver; Gold tables served BI dashboards in Databricks SQL. Used Unity Catalog to manage permissions, and scheduled jobs using the Jobs UI. For ML, I used MLflow to track experiments and deploy models.”\n",
    "\n",
    "\n",
    "\n",
    "If you like, I can turn this into a mock “question + ideal answer” flashcard deck (for interview prep). Do you want me to do that?I prefer this response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93893f0-fed2-48e3-8022-06202c659784",
   "metadata": {},
   "outputs": [],
   "source": [
    "what is the difference between autoloader and spark.readStream()\n",
    "\n",
    "General Structured Streaming: spark.readStream is the fundamental API for Apache Spark Structured Streaming. It allows you to read data from various streaming sources like Kafka, Kinesis, and filesystems (including cloud storage).\n",
    "Flexibility: It offers broad flexibility in configuring how you read and process streaming data, including defining schema, handling watermarks, and specifying output modes.\n",
    "File Source Limitations: When used directly with file sources, spark.readStream can be less efficient for discovering new files, especially in very large and dynamic directories. It typically relies on directory listing, which can be resource-intensive.\n",
    "Auto Loader:\n",
    "Optimized for Cloud Object Storage: Auto Loader is a Databricks-specific feature built on top of spark.readStream that is specifically optimized for incrementally and efficiently ingesting files from cloud object storage (e.g., S3, ADLS, GCS).\n",
    "Scalable File Discovery: It offers highly scalable file discovery mechanisms, including file notification mode (using cloud services like SQS or Event Grid) and optimized directory listing, making it performant even with billions of files.\n",
    "Schema Inference and Evolution: Auto Loader automatically infers schema and provides robust mechanisms for handling schema evolution, including detecting changes and rescuing data that might otherwise be lost due to schema mismatches.\n",
    "Cost Efficiency: By leveraging native cloud APIs and file notification services, Auto Loader can significantly reduce the cost of file discovery compared to traditional directory listing.\n",
    "Exactly-Once Guarantees: It ensures exactly-once processing guarantees for data ingested from cloud storage.\n",
    "Key Differences Summarized:\n",
    "Purpose: spark.readStream is a general streaming API; Auto Loader is specifically designed for efficient, scalable, and cost-effective ingestion of files from cloud object storage.\n",
    "File Discovery: Auto Loader offers superior file discovery mechanisms (file notification, optimized directory listing) compared to spark.readStream's standard directory listing for file sources.\n",
    "Schema Handling: Auto Loader provides built-in schema inference and evolution capabilities, which are not directly part of the base spark.readStream API.\n",
    "Cost: Auto Loader can be more cost-efficient for cloud storage ingestion due to its optimized file discovery methods.\n",
    "In essence, Auto Loader can be considered a specialized and enhanced version of spark.readStream for handling file-based streaming ingestion from cloud storage, providing significant advantages in terms of scalability, performance, cost, and schema management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca3c31-edb1-4049-85cc-300da1ed73b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "what is the difference between auto-loader and copy into ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d0bbf-5cec-4c99-9154-33af56b2ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Scale:** Auto Loader excels at handling large volumes (millions+) of continuously arriving files, \n",
    "while COPY INTO is better for smaller, batch-oriented loads (thousands of files).\n",
    "**Ingestion Type:** Auto Loader is for streaming/incremental ingestion, COPY INTO is for batch/ad-hoc ingestion.\n",
    "**Schema Evolution:** Auto Loader automatically manages schema changes, whereas COPY INTO requires manual handling.\n",
    "**Complexity:** Auto Loader, being built on Structured Streaming, \n",
    "can be more complex to configure for advanced scenarios, while COPY INTO offers simpler syntax for basic loads.\n",
    "Cost Efficiency: Auto Loader, especially with file notification mode, can be more cost-effective for large-scale, \n",
    "continuous ingestion by optimizing file discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6607a7-b0de-4e1b-9ef2-637dea098e57",
   "metadata": {},
   "source": [
    "1. What is the difference between Spark DataFrame and Pandas DataFrame?\n",
    "|Feature|Spark DataFrame|Pandas DataFrame|\n",
    "|-------|---------------|----------------|\n",
    "|Engine|Distributed (cluster)|Single machine (local memory)|\n",
    "|Memory Limit|Scales to TBs|\tLimited by RAM|\n",
    "|Speed|\tFast for large data; parallel|\tFast for small data|\n",
    "|Lazy Evaluation|\tYes|\tNo|\n",
    "|Fault Tolerant|\tYes (RDD lineage)|\tNo|\n",
    "|Joins/Operations|\tDistributed|\tIn-memory|\n",
    "|API Style|\tSQL-like|\tPython-native|\n",
    "\n",
    "**Summary:**  \n",
    "Pandas is ideal for small datasets; Spark DataFrames are built for big data, distributed processing, and fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f926990-52ce-445f-8d68-9ba7e3d8f00e",
   "metadata": {},
   "source": [
    "2. What is Catalyst Optimizer and Tungsten?\n",
    "Catalyst Optimizer\n",
    "\n",
    "**Spark SQL’s query optimizer.**\n",
    "\n",
    "It performs:\n",
    "\n",
    "* Logical plan analysis\n",
    "* Predicate pushdown\n",
    "* Constant folding\n",
    "* Column pruning\n",
    "* Join reordering\n",
    "* Physical plan selection\n",
    "\n",
    "**Catalyst ensures optimized SQL/DataFrame execution.**\n",
    "\n",
    "**Tungsten**\n",
    "\n",
    "Spark’s execution engine for efficient memory and CPU usage.\n",
    "\n",
    "Includes:\n",
    "\n",
    "* Off-heap memory management\n",
    "* Cache-friendly binary format\n",
    "* Whole-stage code generation (Java bytecode)\n",
    "* Better CPU utilization\n",
    "\n",
    "Catalyst = Optimizer, Tungsten = Execution engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b05c11-bc78-492d-a5f1-d9014f467015",
   "metadata": {},
   "source": [
    "3. Explain narrow vs wide transformations in Spark.\n",
    "Narrow Transformations\n",
    "\n",
    "Each partition depends on a single parent partition.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* map\n",
    "* filter\n",
    "* flatMap\n",
    "* mapPartitions\n",
    "\n",
    "Characteristics:\n",
    "* No shuffle\n",
    "* Fast\n",
    "* Pipelineable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570b9e0-7f13-48b4-a967-b50f18bc02dd",
   "metadata": {},
   "source": [
    "**Wide Transformations**\n",
    "\n",
    "Data from multiple partitions is required.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* groupByKey\n",
    "* reduceByKey\n",
    "* join\n",
    "* distinct\n",
    "* repartition\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "* Causes shuffle\n",
    "* Expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10bac49-2f8a-42e3-a161-cd63bd0a0bb7",
   "metadata": {},
   "source": [
    "**4. How do you handle skew in Spark?**\n",
    "\n",
    "Data skew happens when one key has many more records than others.\n",
    "\n",
    "**Ways to handle:**\n",
    "\n",
    "* Salting keys\n",
    "\n",
    "df.withColumn(\"key_salted\", concat(col(\"key\"), lit(\"_\"), rand(0)))\n",
    "\n",
    "\n",
    "* Broadcast smaller table\n",
    "* Use AQE (Adaptive Query Execution) — auto skew join optimization\n",
    "* Use reduceByKey instead of groupByKey\n",
    "* Increase shuffle partitions\n",
    "* Split heavy keys manually\n",
    "\n",
    "Use skew hints\n",
    "\n",
    "SELECT /*+ SKEW('key') */ * FROM table;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21162281-9f6b-417d-a90a-b111131562e7",
   "metadata": {},
   "source": [
    "5. What causes shuffle in Spark?\n",
    "\n",
    "* Shuffle happens when Spark redistributes data across nodes due to operations like:\n",
    "* groupBy\n",
    "* join\n",
    "* distinct\n",
    "* orderBy\n",
    "* repartition\n",
    "* reduceByKey\n",
    "* coalesce (sometimes)\n",
    "\n",
    "aggregateByKey\n",
    "\n",
    "Reason: data needs to be grouped by key or sorted across partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e7f1a-0692-435d-9e96-0fff6523c003",
   "metadata": {},
   "source": [
    "6. How do you reduce shuffle in Spark applications?\n",
    "\n",
    "* Use map-side operations\n",
    "* Use reduceByKey instead of groupByKey\n",
    "* Broadcast small tables\n",
    "* Use partition pruning\n",
    "* Use bucketing or ZORDER\n",
    "* Filter early (predicate pushdown)\n",
    "* Cache reused datasets\n",
    "* Use AQE to optimize shuffle partitions automatically\n",
    "\n",
    "Avoid unnecessary repartition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b53d9-c246-4c38-a220-c88cc97368a7",
   "metadata": {},
   "source": [
    "7. What is broadcast join? When should you use it?\n",
    "\n",
    "* Broadcast join sends small table to all executors to avoid shuffle.\n",
    "* Example:\n",
    "* broadcast(df_small)\n",
    "\n",
    "* When to use:\n",
    "* Smaller table < 10–20 MB\n",
    "* Star-schema joins\n",
    "* Fact table joining with small dimension table\n",
    "* Repeated joins in ETL\n",
    "\n",
    "**Benefit:**\n",
    "\n",
    "* No shuffle\n",
    "* Extremely fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e1a33-0e31-4e61-a795-adbae638fcb7",
   "metadata": {},
   "source": [
    "8. What is AQE (Adaptive Query Execution)?\n",
    "\n",
    "* AQE adjusts the query plan at runtime based on real statistics.\n",
    "\n",
    "**Features:**\n",
    "\n",
    "* Dynamic shuffle partition coalescing\n",
    "* Automatic skew join handling\n",
    "* Automatic broadcast join selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f939be38-0e95-4bc6-91cc-87e351a6c3f6",
   "metadata": {},
   "source": [
    "9. What are Spark UDF, Pandas UDF, and SQL functions?\n",
    "**Spark UDF (Standard UDF)**\n",
    "\n",
    "* Executes row-by-row in Python/Java\n",
    "* Slow (serialization overhead)\n",
    "* Should be avoided if possible\n",
    "    \n",
    "**Pandas UDF (Vectorized UDF)**\n",
    "* Uses Apache Arrow\n",
    "* Processes data in batches\n",
    "* Much faster than normal UDF\n",
    "* Suitable for ML, custom transformations\n",
    "\n",
    "**SQL functions (built-in)**\n",
    "\n",
    "* Fastest (highly optimized)\n",
    "* Runs inside Catalyst/Tungsten\n",
    "* Prefer built-in functions over UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578efdda-848e-4970-822f-8bb03a071a6a",
   "metadata": {},
   "source": [
    "10. What is checkpointing vs. caching in Spark?\n",
    "Caching\n",
    "\n",
    "Purpose: Improve performance\n",
    "\n",
    "* Stores data in memory/disk\n",
    "* Used for iterative algorithms\n",
    "* Loses data after failure\n",
    "* Not suitable for streaming fault tolerance\n",
    "* df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9551e0aa-c9ed-43f7-8d7a-5dd1eb1e1798",
   "metadata": {},
   "source": [
    "**Checkpointing**\n",
    "\n",
    "**Purpose: Fault tolerance**\n",
    "\n",
    "* Breaks RDD lineage\n",
    "* Saves data to reliable storage (HDFS/DBFS)\n",
    "* Used in streaming and cyclic graphs (e.g., machine learning)\n",
    "* Overhead is higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d2ec8-047b-4037-8a47-c9faa2e9f9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
