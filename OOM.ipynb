{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bb0bd-465f-4ccf-89cb-946f7cdbc1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Optimize Memory Configuration\n",
    "a. Increase Executor Memory\n",
    "   --executor-memory 8G\n",
    "b. Increase Driver Memory\n",
    "   --driver-memory 4G\n",
    "c. Tune Memory Fractions\n",
    "   spark.conf.set(\"spark.memory.fraction\", 0.7)\n",
    "   spark.conf.set(\"spark.memory.storageFraction\", 0.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe678b-7ef6-42b3-8127-39929d279047",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Reduce Data Volume per Executor\n",
    "Ensure that no single partition is too large.\n",
    "If your job has very few partitions, each executor may get a huge partition and blow memory.\n",
    "Rule of thumb: 128–256 MB per partition is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783db289-de20-450e-8543-8b0fbcfda7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Avoid Unnecessary Caching\n",
    "Caching holds entire DataFrames in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a1f12-ba58-4151-a3be-182f631a4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Use Disk Storage (instead of Memory)\n",
    "from pyspark import StorageLevel\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bbdecf-bfa2-4d02-87b8-c45bc6f435a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Control Data Skew\n",
    "Skew means one partition gets a lot more data than others (typical in joins or groupBy on skewed keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76871ce-1806-4571-ac26-b9d9d6c92d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Use Broadcast Joins for Small Tables\n",
    "Avoids shuffle and heavy memory usage during join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc644aa2-e143-4b57-8baa-547524dd24f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Reduce Shuffle Pressure\n",
    "Shuffles create large temporary data on disk/memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481076ec-3474-4bd5-a7f6-ac197bbe5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Avoid Collecting to Driver\n",
    "df.collect() and df.toPandas() load all data into driver memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78dbea4-c836-4a30-a8d4-7c81410486fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Enable Off-Heap Memory (Optional Advanced) \n",
    "If you want to let Spark spill large data to off-heap (outside JVM heap):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8625987-49a6-4761-913e-2973ee7dbbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Use Compression for Shuffle Files\n",
    "    Compressing shuffle data reduces memory and disk usage:\n",
    "    spark.conf.set(\"spark.shuffle.compress\", True)\n",
    "    spark.conf.set(\"spark.shuffle.spill.compress\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2002a-257a-41db-af73-ee059eea0206",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Monitor Spark UI\n",
    "\n",
    "Use Spark UI (port 4040):\n",
    "Storage tab → cached data size.\n",
    "Executors tab → memory used per executor.\n",
    "SQL tab → large shuffle read/write sizes → potential OOM areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd8715-fbf1-4c32-8dd9-68d676f3a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. Tune Garbage Collection\n",
    "Large Java heaps (8+ GB) can cause long GC pauses.\n",
    "--conf \"spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+PrintGCDetails\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
