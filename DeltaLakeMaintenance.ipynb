{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d598a59-63a3-46c5-8e7b-86029b77dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta Lake Maintenance is divided into two parts.\n",
    "1. Logical Rewrite \n",
    "2. Physical Rewrite "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5fb2b-b6e3-4f44-a20e-bb60cfe407d1",
   "metadata": {},
   "source": [
    "In Databricks, managing a Delta table is essentially about optimizing how data is laid out so that the Spark engine can find it faster. You've correctly categorized these into Logical (how we organize the structure) and Physical (how we actually move the bytes on disk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d6848-119b-40d2-852c-a962612de260",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Logical Rewrite is divided into two parts\n",
    "   a) Partitioning \n",
    "   b) Liquid Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba65309-2a77-461f-8197-d21a2055d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Physical Rewrite is divided into three parts\n",
    "   a) Optimize\n",
    "   b) Z-order\n",
    "   c) Vaccum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6544db-f625-4750-bce0-f0c92bae402d",
   "metadata": {},
   "source": [
    "Logical Rewrite: The Structural Blueprint\n",
    "A logical rewrite doesn't necessarily change the data itself immediately, but it changes the rules by which data is organized. It’s about defining the \"containers\" that data should live in.\n",
    "\n",
    "a) Partitioning (The Traditional Way)\n",
    "Partitioning involves physically separating data into folders based on a specific column (e.g., Year or Country).\n",
    "\n",
    "* How it works: When you query WHERE Year = 2024, Spark skips every folder except the one labeled 2024.\n",
    "\n",
    "* The Downside: It is rigid. If you partition by a column with too many unique values (high cardinality), you end up with \"tiny file syndrome,\" which slows down performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373aa4fa-3921-419a-b025-5f48f7110a3f",
   "metadata": {},
   "source": [
    "b) Liquid Clustering (The Modern Way)\n",
    "Liquid Clustering is the successor to partitioning. It is \"flexible\" rather than \"fixed.\"\n",
    "\n",
    "**How it works:** Instead of hard-coded folders, Databricks uses a clustering key to group similar data together within files.\n",
    "\n",
    "**Why it's \"Logical\":** You can change the clustering keys without rewriting your entire table. It automatically adjusts as data grows, avoiding the pitfalls of over-partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574835d-7cba-4c3b-9726-054b877005bf",
   "metadata": {},
   "source": [
    "2. Physical Rewrite: The Disk Cleanup\n",
    "Physical rewrites involve changing the actual Parquet files stored in your cloud storage (S3/ADLS). Delta Lake creates new files and marks old ones for deletion to improve performance.\n",
    "\n",
    "a) Optimize (Compaction)\n",
    "Over time, many small JSON or Parquet files accumulate. Optimize takes those 1,000 tiny files and rewrites them into a few large, \"right-sized\" files (usually ~1GB). This reduces the \"I/O overhead\" of opening and closing files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5448dc7-49fc-465f-9fe4-be9dca515276",
   "metadata": {},
   "source": [
    "b) Z-Order (Data Skipping)\n",
    "Often run alongside Optimize, Z-Ordering rearranges the data inside the files.\n",
    "\n",
    "**How it works:** It maps multi-dimensional data into one dimension. If you Z-Order by CustomerID, the records for \"Customer A\" are physically stored next to each other on the disk. This allows Spark to \"skip\" entire chunks of a file that don't match your query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e9adf9-1949-42fd-930b-bc8b48446701",
   "metadata": {},
   "source": [
    "c) Vacuum\n",
    "When you perform an Optimize or an Update, Delta doesn't immediately delete the old files (to allow for Time Travel). Vacuum is the cleanup crew.\n",
    "\n",
    "How it works: It permanently removes files that are no longer referenced by the current version of the table and are older than a retention period (default is 7 days). This saves money on storage costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95727208-8f53-4577-bd2a-85e1ef6f4a8c",
   "metadata": {},
   "source": [
    "Moving from traditional partitioning to Liquid Clustering is one of the best \"quality of life\" upgrades in Databricks because it removes the headache of managing **partition evolution**.\n",
    "\n",
    "To convert a table, the process depends on whether you are creating a new table from existing data or altering an existing Delta table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f3e3a6-3c27-49f7-b03b-3c9abf1a2afc",
   "metadata": {},
   "source": [
    "1. The SQL Approach (Most Common)\n",
    "If you have an existing partitioned table and want to enable Liquid Clustering, you use the CLUSTER BY syntax.\n",
    "\n",
    "To convert an existing table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0fbf21-caf1-4395-908b-f4e18cca383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- 1. Change the table property to use clustering instead of partitioning\n",
    "ALTER TABLE my_catalog.my_schema.sensor_data\n",
    "CLUSTER BY (device_id, sensor_type);\n",
    "\n",
    "-- 2. Trigger the physical rewrite to reorganize the data\n",
    "OPTIMIZE my_catalog.my_schema.sensor_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e299d-b804-41c9-8229-ddb12cadfc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "To create a fresh table with Liquid Clustering:\n",
    "\n",
    "CREATE TABLE my_catalog.my_schema.new_table\n",
    "(id INT, ts TIMESTAMP, val DOUBLE)\n",
    "USING DELTA\n",
    "CLUSTER BY (id); -- No 'PARTITIONED BY' needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed150736-9a31-46d3-84e8-8dc7634c654d",
   "metadata": {},
   "source": [
    "**Important Transition Rules**\n",
    "\n",
    "* When you move to Liquid Clustering, there are a few \"under the hood\" changes to be aware of:\n",
    "* Dropping Partitions: You cannot use PARTITIONED BY and CLUSTER BY on the same table. When you alter a table to use clustering, Databricks stops using the old partition folders for new data.\n",
    "* The \"Optimize\" Trigger: Simply running the ALTER TABLE command is a Logical Rewrite. The data isn't actually moved until you run OPTIMIZE. That is the Physical Rewrite that aligns the files to your new keys.\n",
    "\n",
    "Key Selection: Unlike Z-Ordering (where order matters), the order of columns in CLUSTER BY does not matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd4962-73d5-483d-b079-a4199ae812c6",
   "metadata": {},
   "source": [
    "If you are managing a large-scale Lakehouse, manually running DESCRIBE DETAIL on every table isn't feasible.\n",
    "\n",
    "Using the Information Schema allows you to treat your metadata like a database, letting you audit your entire environment in seconds to see which tables have been modernized and which are still using legacy partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58810c86-6e39-4cf9-a5b8-cd7aa7fd7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "    table_catalog, \n",
    "    table_schema, \n",
    "    table_name, \n",
    "    table_type,\n",
    "    created_by\n",
    "FROM \n",
    "    system.information_schema.tables\n",
    "WHERE \n",
    "    -- This filters for tables where clustering is enabled in the metadata\n",
    "    array_contains(table_features, 'clustering') \n",
    "    AND table_schema != 'information_schema';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c07e4-26f6-48be-9ecb-59d4db24f524",
   "metadata": {},
   "source": [
    "I would definitely like to see that! If you are managing a large-scale Lakehouse, manually running DESCRIBE DETAIL on every table isn't feasible.\n",
    "\n",
    "Using the Information Schema allows you to treat your metadata like a database, letting you audit your entire environment in seconds to see which tables have been modernized and which are still using legacy partitioning.\n",
    "\n",
    "1. The SQL Audit Query\n",
    "In Unity Catalog, the tables system view contains a column for table_features. Liquid clustering is stored as a specific feature property. You can use the following query to find every clustered table in a specific catalog:\n",
    "\n",
    "SQL\n",
    "SELECT \n",
    "    table_catalog, \n",
    "    table_schema, \n",
    "    table_name, \n",
    "    table_type,\n",
    "    created_by\n",
    "FROM \n",
    "    system.information_schema.tables\n",
    "WHERE \n",
    "    -- This filters for tables where clustering is enabled in the metadata\n",
    "    array_contains(table_features, 'clustering') \n",
    "    AND table_schema != 'information_schema';\n",
    "\n",
    "2. Finding Tables That AREN'T Clustered (Migration List)\n",
    "Often, the more helpful audit is finding the tables that should be clustered but aren't. This query identifies large Delta tables that are still using traditional partitioning, which makes them prime candidates for a migration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd0504-7266-460c-89f9-ce500522374a",
   "metadata": {},
   "source": [
    "Building a Clustering Coverage Dashboard is the \"pro\" way to manage a Databricks environment. Instead of running manual audits, you get a real-time visual of your technical debt and optimization progress.\n",
    "\n",
    "To build this, we’ll use the Databricks SQL (DB SQL) workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de893d0-dee1-4af0-a9cd-161fc20b4974",
   "metadata": {},
   "source": [
    "1. The \"Clustering vs. Partitioning\" Pie Chart     \n",
    "First, create a visualization that shows the percentage of your tables that have adopted Liquid Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e7215-51b4-496b-bae8-4a60f478da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "    CASE \n",
    "        WHEN array_contains(table_features, 'clustering') THEN 'Liquid Clustered'\n",
    "        WHEN cardinality(partition_columns) > 0 THEN 'Legacy Partitioned'\n",
    "        ELSE 'Flat (No Partition/Cluster)'\n",
    "    END AS Table_Strategy,\n",
    "    COUNT(*) AS Table_Count\n",
    "FROM system.information_schema.tables\n",
    "WHERE table_schema NOT IN ('information_schema', 'sys')\n",
    "GROUP BY 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a07dc7-beaa-41ab-9799-3c3ec03b415b",
   "metadata": {},
   "source": [
    "Visualization: Choose a Pie Chart. This gives leadership a quick look at how the migration to Delta Lake 3.x features is going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5503d9-e070-45ab-b5f6-c5220fa77e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Note: This requires access to the 'system.storage' schema to join size data\n",
    "SELECT \n",
    "    t.table_schema,\n",
    "    t.table_name,\n",
    "    format_number(s.total_size_mb / 1024, 2) as size_gb\n",
    "FROM system.information_schema.tables t\n",
    "JOIN system.storage.table_size_metrics s -- Joins to get physical size\n",
    "  ON t.table_id = s.table_id\n",
    "WHERE NOT array_contains(t.table_features, 'clustering')\n",
    "ORDER BY s.total_size_mb DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca38319-fc76-431a-b591-4bbd97d02355",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization: Choose a Counter or a Table widget."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d67b27-2858-43c9-a46d-5003d74c6743",
   "metadata": {},
   "source": [
    "3. The \"Optimization Health\" Line Chart\n",
    "To see if your Physical Rewrites are actually happening, you can track the frequency of OPTIMIZE commands across your catalog over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56123911-fdbd-4212-b24c-e6407456a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "    date_trunc('day', event_time) AS audit_date,\n",
    "    COUNT(*) AS optimize_runs\n",
    "FROM system.access.audit_log\n",
    "WHERE service_name = 'unityCatalog' \n",
    "  AND action_name = 'optimizeTable'\n",
    "GROUP BY 1\n",
    "ORDER BY 1 DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b24ae-0dc3-4b6d-beac-53aea61a3758",
   "metadata": {},
   "source": [
    "Visualization: Choose a Line Chart. If this line is flat, it means your tables are logically clustered but you aren't running the physical rewrites necessary to gain performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b98ff-f785-440e-a90c-5d541f5a419e",
   "metadata": {},
   "source": [
    "|Widget|Insight|Benefit|\n",
    "|-------|-------|--------|\n",
    "|Pie Chart|Coverage %|High-level migration tracking|\n",
    "|Table List|Migration Targets|\"Identifies \"\"heavy hitters\"\" for ALTER TABLE.\"|\n",
    "|Line Chart|Maintenance Activity|Ensures OPTIMIZE jobs are actually running.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4dd2e-688d-4b17-853c-0c674be39d42",
   "metadata": {},
   "source": [
    "**How to Deploy**  \n",
    "* Open SQL Warehouses and ensure one is running.\n",
    "* Go to Dashboards > Create Dashboard.\n",
    "* Add the queries above as Widgets.\n",
    "* Set a Refresh Schedule (e.g., every Monday morning) to keep the data fresh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca7cd1-fa3a-4712-a232-2c4b0dc54b36",
   "metadata": {},
   "source": [
    "The Automation Script: \"The Migration Loop\"   \n",
    "This script uses the databricks.sdk or standard PySpark to query the Information Schema, identify the target tables, and then execute the ALTER and OPTIMIZE commands sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57042208-557a-4bfa-a01d-0856c75926c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary Spark functions\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Define your target catalog and the clustering column logic\n",
    "# (In this example, we'll cluster by 'device_id' if it exists, or 'id')\n",
    "TARGET_CATALOG = \"main\"\n",
    "\n",
    "def migrate_to_liquid_clustering():\n",
    "    # 2. Query Information Schema for the 'Migration List'\n",
    "    # We look for Managed Delta tables that are NOT yet clustered\n",
    "    migration_targets = spark.sql(f\"\"\"\n",
    "        SELECT table_schema, table_name \n",
    "        FROM {TARGET_CATALOG}.information_schema.tables \n",
    "        WHERE table_type = 'MANAGED' \n",
    "        AND NOT array_contains(table_features, 'clustering')\n",
    "        AND table_schema NOT IN ('information_schema', 'sys')\n",
    "        LIMIT 10\n",
    "    \"\"\").collect()\n",
    "\n",
    "    if not migration_targets:\n",
    "        print(\"No tables found for migration. Your Lakehouse is up to date!\")\n",
    "        return\n",
    "\n",
    "    for row in migration_targets:\n",
    "        full_table_name = f\"{TARGET_CATALOG}.{row.table_schema}.{row.table_name}\"\n",
    "        print(f\"Starting migration for: {full_table_name}\")\n",
    "\n",
    "        try:\n",
    "            # 3. Step 1: Logical Rewrite (Define Clustering Key)\n",
    "            # You can customize the clustering key logic here\n",
    "            spark.sql(f\"ALTER TABLE {full_table_name} CLUSTER BY (id)\")\n",
    "            print(f\"  - Logical Rewrite Complete: Clustering key set to 'id'\")\n",
    "\n",
    "            # 4. Step 2: Physical Rewrite (Compaction and Clustering)\n",
    "            spark.sql(f\"OPTIMIZE {full_table_name}\")\n",
    "            print(f\"  - Physical Rewrite Complete: Data reorganized on disk\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  - Error migrating {full_table_name}: {e}\")\n",
    "\n",
    "# Run the migration\n",
    "migrate_to_liquid_clustering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ed4a4-18d5-490f-be40-ae5039be4a31",
   "metadata": {},
   "source": [
    "In Databricks and Spark SQL, table_features is a column that stores an Array of strings. Because it is a list rather than a single value, you cannot use a simple equals sign (=) to query it. This is where array_contains comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2585d9-0c68-4205-9574-3803fe49a907",
   "metadata": {},
   "source": [
    "1. What is array_contains?\n",
    "The array_contains(column, value) function is a boolean check. It looks inside a collection (the array) and returns True if the specific value exists anywhere in that list, and False if it does not.\n",
    "\n",
    "In the context of your audit query:\n",
    "\n",
    "The Column (table_features): A list of all advanced Delta Lake capabilities enabled for that specific table.\n",
    "\n",
    "The Value ('clustering'): The specific string that identifies Liquid Clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
